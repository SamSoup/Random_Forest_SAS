{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "chronic-universe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library preparation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "forward-papua",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom I/O functions\n",
    "def read_labels(filename):\n",
    "    \"\"\"\n",
    "    Reads the labels.csv file containing info on\n",
    "    which samples contain SAS or not\n",
    "\n",
    "    Input: \n",
    "        filename: file path to labels.csv\n",
    "    Output:\n",
    "        y: a numpy array of labels \"Yes\" vs \"No\"\n",
    "    \"\"\"\n",
    "\n",
    "    y = pd.read_csv(filename)\n",
    "    y = y.sort_values(by=\"indices\").drop(\"indices\", axis=1).to_numpy()\n",
    "\n",
    "    return y\n",
    "\n",
    "def read_samples(files, filePath):\n",
    "    \"\"\"\n",
    "    Read in the output files from SA.wrapper.r\n",
    "    and save it as a compressed file using numpy\n",
    "    \n",
    "    Input:\n",
    "        files: an iterable of names that indicate which output\n",
    "        files to read; len(samples) must be greater than or equal to one\n",
    "        \n",
    "        filePath: path to the folder containing all output files\n",
    "        \n",
    "    Output:\n",
    "        a numpy ndarray of all output files concatenated together\n",
    "    \"\"\"\n",
    "\n",
    "    iterator = iter(files)\n",
    "    \n",
    "    # grab the first output data and use it as a baseline\n",
    "    firstFile = next(iterator)\n",
    "    fileStart = os.path.join(filePath, firstFile)\n",
    "    X = pd.read_table(fileStart, header=None).to_numpy()\n",
    "    rows, columns = X.shape\n",
    "\n",
    "    # We now concatenate all samples together\n",
    "    # Note: THIS COULD TAKE A WHILE! (only run once)\n",
    "    start = time.time()\n",
    "    for fileName in iterator:\n",
    "        fullPath = os.path.join(filePath, fileName)\n",
    "        sample = pd.read_table(fullPath, header=None).to_numpy()\n",
    "        X = np.concatenate((X, sample), axis=0)\n",
    "    end = time.time()\n",
    "    print('Time taken to concatenate files:', end - start)\n",
    "\n",
    "    # reshape to samples, rows, columns\n",
    "    X = X.reshape(-1, rows, columns)\n",
    "\n",
    "    return X\n",
    "\n",
    "def clean_data(X, y, threshold):\n",
    "    \"\"\"\n",
    "    Ensure no element is nan or infinity, otherwise scikit-learn \n",
    "    will not like that.\n",
    "    \n",
    "    If samples with nan or infinity is below a threshold amount of the \n",
    "    total data/samples in X, then we drop them\n",
    "    \n",
    "    Input: \n",
    "        X: a three dimensional numpy array\n",
    "            + Assume first dimension = # of samples\n",
    "            + Assume second dimension = # of rows in each sample\n",
    "            + Assume third dimension = # of columns in each sample\n",
    "        y: the corresponding label values for X\n",
    "        \n",
    "    Output:\n",
    "        X_clean: X cleaned if mising data is below threshold, otherwise X\n",
    "        y_clean: y cleaned if missing data is below threshold, otherwise y\n",
    "        samples_clean: number of samples left after cleaning (max is X.shape[0])\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(X.shape) == 3\n",
    "    assert y.shape[0] == X.shape[0]\n",
    "    samples = X.shape[0]\n",
    "    print(\"Dected\", samples, \"samples total\")\n",
    "\n",
    "    has_NAN = np.any(np.isnan(X))\n",
    "    samples_with_nans = set()\n",
    "    if has_NAN:\n",
    "        print(\"We got NaNs, identifying location of NaNs...\")\n",
    "        indices_nan = np.argwhere(np.isnan(X))\n",
    "        samples_with_nans.update(set(indices_nan[:, 0]))\n",
    "        columns_with_nans = set(indices_nan[:, 2])\n",
    "        print(f\"Attributes with NaNs are: {columns_with_nans}\")\n",
    "        # print(f\"Samples with NaNs are: {samples_with_nans}\")\n",
    "        print(f\"There are \", len(samples_with_nans), \" Samples with NANs\")\n",
    "\n",
    "    has_Inf = np.any(np.isinf(X))\n",
    "    samples_with_Infs = set()\n",
    "    if has_Inf:\n",
    "        print(\"We got Infinities, identifying location of Infs...\")\n",
    "        indices_Infs = np.argwhere(np.isinf(X))\n",
    "        samples_with_Infs.update(set(indices_Infs[:, 0]))\n",
    "        columns_with_Infs = set(indices_Infs[:, 2])\n",
    "        print(f\"Attributes with Infs are: {columns_with_Infs}\")\n",
    "        # print(f\"Samples with Infs are: {samples_with_nans}\")\n",
    "        print(f\"There are \", len(samples_with_Infs), \" Samples with Infs\")\n",
    "\n",
    "    # Attempting to clean the dataset from NaNs and Infs\n",
    "    bug_samples = set()\n",
    "    bug_samples.update(samples_with_nans)\n",
    "    bug_samples.update(samples_with_Infs)\n",
    "    print(f\"There are a total of {len(bug_samples)} containing Infs or NaNs\")\n",
    "    bug_sample_l = list(bug_samples)\n",
    "    bug_sample_l.sort()\n",
    "    print(\"Deleting samples \", bug_sample_l)\n",
    "\n",
    "    if len(bug_samples) < threshold*samples:\n",
    "        print(f\"Less than {threshold*100}% of Samples, dropping...\")\n",
    "        X_clean = np.delete(X, bug_sample_l, axis = 0)\n",
    "        y_clean = np.delete(y, bug_sample_l, axis = 0)\n",
    "        samples_clean = samples - len(bug_sample_l)\n",
    "        print(f\"After dropping, we have {samples_clean} samples left\")\n",
    "    else:\n",
    "        print(\"Too many Samples to drop, can't clean!\")\n",
    "        \n",
    "    return X_clean, y_clean, samples_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-clothing",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "\n",
    "We first read in training samples using an iterable of filenames and 'compress' down to one dimesnion using `reshape` so that RandomForest can evaluate it correctly\n",
    "\n",
    "The training samples consist of a 50-50 split betwee neutral samples and SAS samples of shape (160, 2) per sample. \n",
    "I choose the 50-50 ratio because **class imabalnce** is often a big headache in model fitting and for the sake of training, a 50-50 split will best enable the classifier to capture the signals of SAS. \n",
    "\n",
    "The attribute key is as belows:\n",
    "\n",
    "1. Maximum fst in a 2.5 rho window\n",
    "2. Mean-squared error for the highest fst peak in a window\n",
    "\n",
    "`y` will contain label information for if sample i has SAS or not of shape (# of samples, 1) with one aitribute:\n",
    "\n",
    "1. SAS (Yes or No)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "breeding-velvet",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Yes']\n",
      " ['Yes']\n",
      " ['Yes']\n",
      " ['No']\n",
      " ['No']]\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "y_train = read_labels(\"./GenInput/input/label.csv\")\n",
    "print(y_train[:5])\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "southeast-involvement",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to concatenate files: 18.96846079826355\n",
      "(10000, 160, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.39257336, 0.14608226],\n",
       "       [0.23921614, 0.0362196 ],\n",
       "       [0.39257336, 0.1809441 ],\n",
       "       [0.35295858, 0.15712493],\n",
       "       [0.35295858, 0.19305147],\n",
       "       [0.24498803, 0.05261772],\n",
       "       [0.24498803, 0.0496599 ],\n",
       "       [0.24498803, 0.0707043 ],\n",
       "       [0.24498803, 0.05170502],\n",
       "       [0.24498803, 0.06001001]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generator, actual cost = O(1)\n",
    "files = (\"output_\" + str(i) for i in range(1, 10000+1))\n",
    "filePath = \"./GenInput/output\"\n",
    "\n",
    "X_train = read_samples(files, filePath)\n",
    "print(X_train.shape)\n",
    "X_train[0][:10, ] # check first 10 rows of first sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "abandoned-garden",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dected 10000 samples total\n",
      "There are a total of 0 containing Infs or NaNs\n",
      "Deleting samples  []\n",
      "Less than 1.0% of Samples, dropping...\n",
      "After dropping, we have 10000 samples left\n"
     ]
    }
   ],
   "source": [
    "X_clean, y_clean, samples_clean = clean_data(X_train, y_train, threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "injured-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the result into a numpy object\n",
    "np.savez_compressed('./data/train.npz', \n",
    "                    X_train=X_clean, y_train=y_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-invention",
   "metadata": {},
   "source": [
    "## Small Test Data Batch\n",
    "\n",
    "I then asked the simulation for a 50-50 split, 1000 sample test set that's not used in the model building process at all for an earnest attempt at out-of-sample prediction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "knowing-survey",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Yes']\n",
      " ['No']\n",
      " ['Yes']\n",
      " ['Yes']\n",
      " ['Yes']]\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "y_test = read_labels(\"./GenInputTest/input/label.csv\")\n",
    "print(y_test[:5])\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "silent-lighter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to concatenate files: 0.725487470626831\n",
      "(1000, 160, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.43241981, 0.26219987],\n",
       "       [0.43241981, 0.32465513],\n",
       "       [0.35295858, 0.24039368],\n",
       "       [0.43241981, 0.31260821],\n",
       "       [0.43241981, 0.3246583 ],\n",
       "       [0.43241981, 0.39181164],\n",
       "       [0.43241981, 0.40586182],\n",
       "       [0.43241981, 0.39830596],\n",
       "       [0.43241981, 0.43595325],\n",
       "       [0.43241981, 0.37849837]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generator, actual cost = O(1)\n",
    "files = (\"output_\" + str(i) for i in range(1, 1000+1))\n",
    "filePath = \"./GenInputTest/output\"\n",
    "\n",
    "X_test = read_samples(files, filePath)\n",
    "print(X_test.shape)\n",
    "X_test[0][:10, ] # check first 10 rows of first sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "worldwide-desire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dected 1000 samples total\n",
      "There are a total of 0 containing Infs or NaNs\n",
      "Deleting samples  []\n",
      "Less than 1.0% of Samples, dropping...\n",
      "After dropping, we have 1000 samples left\n",
      "\n",
      "\n",
      "(1000, 160, 2)\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "X_test_clean, y_test_clean, samples_test = clean_data(\n",
    "    X_test, y_test, threshold=0.01)\n",
    "print(\"\\n\")\n",
    "print(X_test_clean.shape)\n",
    "print(y_test_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "former-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the result into a numpy object\n",
    "np.savez_compressed('./data/test.npz', X_test=X_test_clean, y_test=y_test_clean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
